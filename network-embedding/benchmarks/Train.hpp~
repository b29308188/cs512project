#pragma once
#include "utils.h"
#include <fstream>
#include <iostream>
#include <string>
#include <vector>
#include <map>
#include <tuple>
#include <random>
using namespace std;


enum class SamplingMethod {
  UniformDistribution,
  BernouliDistribution,
  GaussianDistribution
};


// (head, tail, relationship)
using triplet_t = tuple<size_t, size_t, size_t>;

class Train {
public:
  Train(size_t e_dimension, size_t r_dimension, double learning_rate, double margin,SamplingMethod s);
  /*
   * Read pre-processed data
   */
  void readData();

  /*
   * Traverse all relationship and update   
   */
  void run();

  /*
   * Write to file
   */
  void writeData();
/*
 * private functions
 */
private:
  // update a mini batch
  void batchUpdate();
  // update weight
  void weightUpdate();

  void computeGraident();

/*
 * Data
 */
private:

  size_t e_dimension_ = 0;
  size_t r_dimension_ = 0;
  double learning_rate_ 0.1;
  double margin_ = 1;

  size_t num_batches_ = 100;
  size_t num_epoch_ = 1000;
  SamplingMethod sampling_m_ = UniformDistribution;
  
  vector<triplet_t> triplets_; 
  vector<vector<int> > feature_;
  vector<vector<double> > relation_mat_, entity_mat_;
  vector<vector<double> > relation_tmp_, entity_tmp_;

  string relation_file_name = "relation_mat";  
  string entity_file_name = "entity_vec";

  uniform_int_distribution<> entity_sampler_;
  uniform_int_distribution<> triplet_sampler_;
};

Train::Train(size_t e_dimension, size_t r_dimension, double learning_rate, double margin,SamplingMethod s):
  e_dimension_(e_dimension), r_dimension_(r_dimension), learning_rate_(learning_rate), margin_(margin), sampling_m_(s)
{
}

void Train::run() {
  // prepare random device
  entity_sampler_ = uniform_int_distribution<>(0, entity_mat_.size());
  triplet_sampler_ = unfiorm_int_distribution<>(0, triplets_.size())

  const int batchsize = triplets__.size();
  double loss = 0.0;

  // for each epoch
  for ( size_t epoch_idx = 0; epoch_idx < num_epoch_; epoch_idx++ ){

    loss = 0.0;
    // for each batch
    for ( size_t batch_idx = 0; batch_idx < num_batches_; batch_idx ++) {
      // keep the current value of realtion vectors and entity vectors
      relation_tmp_ = relation_mat_;
      entity_tmp_ = entity_mat_;
      batchUpdate(generator, entity_sampler, triplet_sampler);
    } //end for each batch 
    // update the embedding after each batch update
    relation_mat_ = relation_tmp_;
    entity_mat_ = entity_tmp_;

    cout << "Epoch: " << epoch_idx << " " << loss << endl;
  } // end for each epoch
}


void Train::batchUpdate(const mt19937& gen, 
    const uniform_int_distribution<>& entity_sampler, 
    const uniform_int_distribution<>& triplet_sampler) {

  for ( size_t idx; idx < batchsize; idx ++ ) {
    // sample a triplet
    auto triplet_idx = triplet_sampler(gen);
    // get the head and the tail
    auto head_id = std::get<0> triplets_[triplet_idx];
    auto tail_id = std::get<1> triplets_[triplet_idx];
    auto relation_id = std::get<2> tripletes_[triplet_idx];

    // throw a coin to decide to remove head_id or remove tail_id
    auto coin_num = Utils::coin(Utils::gen);
    const auto& exist_entities = coin_num? relation_id_table[ make_pair(head_id, relation_id)]:inverse_relation_id_table[make_pair(tail_id, relation_id)]
    size_t replace_id = negative_sample(exist_entities, entity_sampler);
    // replace tail
    if (coin_num == 0) 
      weightUpdate(head_id, tail_id, relation_id, head_id, replace_id, relation_id);
    // replace head
    else 
      weightUpdate(head_id, tail_id, relation_id, replace_id, tail_id, relation_id);

    Utils::normalize(relation_tmp_[relation_id]);
    Utils::normalize(entity_tmp_[head_id]);
    Utils::normalize(entity_tmp_[tail_id]);
    Utils::normalize(entity_tmp_[replace_id]);
  }
}

void weightUpdate(size_t head_id, size_t tail_id, size_t relation_id, 
                  size_t comp_head_id, size_t comp_tail_id, size_t comp_relation_id) {
  auto sum1 = computeSum( head_id, tail_id, relation_id);
  auto sum2 = computeSum( comp_head_id, comp_tail_id, comp_relation_id);
  if (sum1+margin_ > sum2) {
    loss += margin + sum1 - sum2;
    computeGradient( head_id, tail_id, relation_id, comp_head_id, comp_tail_id, comp_relation_id);
  }
}

void 

void Train::writeData() {
  // open file for writing
  ofstream relation_file, entity_file;
  relation_file.open(relation_file_name, ios::out|ios::trunc);
  entity_file.open(relation_file_name, ios::out|ios::trunc);
  // print out each realtion vector
  for( auto relation_vec : relation_mat_) {
    for ( auto element : relation_vec)
      relation_file << setprecision(7) << element << '\t' ;
    relation_file << endl;
  }
  // print out each entity vector
  for ( auto entity_vec : entity_mat_ ) {
    for ( auto element : entity_vec) 
      entity_file << setprecision(7) << element << '\t';
    entity_file << endl;
  }
}
